{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import threading\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server class\n",
    "class Server:\n",
    "    # Constructor\n",
    "    def __init__(self, limit):\n",
    "        self.limit = limit                      # limit for number of workers\n",
    "        self.current_workers = 0                # current number of workers who are connected\n",
    "        self.lock = threading.Lock()            # mutex lock for when max workers reached\n",
    "        self.stop_event = threading.Event()     # enables us to stop the server and workers after a timelimit so we can print worker metrics\n",
    "        self.state = {}\n",
    "        self.all_states = []\n",
    "\n",
    "    # A worker connects\n",
    "    def connect(self):\n",
    "        with self.lock:                             # mutex for multithreading\n",
    "            if self.current_workers < self.limit:   # check current num workers against worker limit\n",
    "                self.current_workers += 1           # increment number of current workers\n",
    "                return True                         # successful connection\n",
    "            return False                            # connection refused\n",
    "\n",
    "    # A worker disconnects\n",
    "    def disconnect(self):\n",
    "        with self.lock:                 # mutex for multithreading\n",
    "            self.current_workers -= 1   # decrement number of current workers\n",
    "\n",
    "    # For stopping server after we have run the sim for the desired amount of time\n",
    "    def stop(self):\n",
    "        self.stop_event.set()   # stops the threading\n",
    "\n",
    "# Worker class\n",
    "class Worker(threading.Thread):\n",
    "    # Constructor\n",
    "    def __init__(self, server, priority='normal', identifier=-1, backoff_time=2):\n",
    "        super().__init__()\n",
    "        self.server = server              # server instance for connecting/deconnecting\n",
    "        self.priority = priority          # priority level of worker\n",
    "        self.backoff_time = backoff_time  # initial backoff time\n",
    "        self.init_backoff_time = backoff_time\n",
    "        self.total_work_time = 0          # track total work time to see how starved/successful worker is\n",
    "        self.work_time = 0                # track work time each connection/work/disconnection cycle\n",
    "        self.backoff_count = 0            # track total number of backoffs (failed connection attempts)\n",
    "        self.connection_attempts = 0      # trac total number of connection attempts\n",
    "        self.connection_probability = 0   # Connection probability = 1 - (backoff_count / connection attempts)\n",
    "        self.id = identifier\n",
    "\n",
    "    # Worker loop for simulating connection/work/disconnection cycle\n",
    "    def run(self):\n",
    "        while not self.server.stop_event.is_set():  # Until the threading event ends\n",
    "            connected = self.server.connect()       # Attempt connection\n",
    "            self.connection_attempts += 1\n",
    "            if connected:                           # If connection successful\n",
    "                #print(f\"{self.name}: Connected to server.\")\n",
    "                self.backoff_time = self.init_backoff_time  # Reset backoff time\n",
    "                self.server.state[f\"w{self.id}_backoff\"] = -1\n",
    "                try:\n",
    "                    while not self.server.stop_event.is_set():  # If threading event isn't ending\n",
    "                        time.sleep(1)                           # Sleep 1 second every cycle while connected, this second represents a second of work performed on server\n",
    "                        self.work_time += 1                     # Track work\n",
    "                        if random.random() < 0.25:              # Chance to disconnect every 1 second cycle of work\n",
    "                            break\n",
    "                finally:\n",
    "                    self.server.disconnect()                    # Disconnect from server\n",
    "                    self.total_work_time += self.work_time      # Update work total\n",
    "                    self.work_time = 0                          # Reset cycle tracker\n",
    "                    time.sleep(random.uniform(1, 3))            # Backoff for some time so the worker doesn't immediately reconnect and starve out the others\n",
    "            else:                                   # Connection Failed\n",
    "                self.backoff_count += 1\n",
    "                self.handle_backoff()               # Handle backoff based on priority of worker\n",
    "\n",
    "    # Handle worker backoff after connection refusal based on the priority of the worker\n",
    "    def handle_backoff(self):\n",
    "        self.server.state[f\"w{self.id}_backoff\"] = int(self.backoff_time) + 1\n",
    "        time.sleep(self.backoff_time)\n",
    "        self.backoff_time = min(self.backoff_time * 2, self.init_backoff_time)  # double backoff time every backoff\n",
    "\n",
    "    # Get the total time a worker has performed work while connected to server\n",
    "    def get_total_work_time(self):\n",
    "        return self.total_work_time     # Simpy return tracker variable\n",
    "\n",
    "# Function for lifecycle of server. For example, when we time.sleep(300), we get 5 minutes of server life\n",
    "def server_lifecycle(server, workers, lifecycle=300):\n",
    "    # Run server for 5 minutes\n",
    "    for t in range(lifecycle):\n",
    "        time.sleep(1)\n",
    "        for key in server.state:\n",
    "            if server.state[key] > 0:\n",
    "                server.state[key] = server.state[key] - 1\n",
    "        server.all_states.append(server.state.copy())\n",
    "\n",
    "    server.stop()           # After the <lifecycle> seconds have elapsed, stop the server\n",
    "    for worker in workers:  # This loop prints the worker metrics\n",
    "        worker.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate server\n",
    "def simulate(server, worker_count=20, server_life=30, backoff_times=[1, 2, 4]):\n",
    "    workers = []\n",
    "    for i in range(worker_count):\n",
    "        if i % 3 == 0:\n",
    "            worker = Worker(server, priority='high', identifier=i, backoff_time=backoff_times[0])\n",
    "        elif i % 3 == 1:\n",
    "            worker = Worker(server, priority='normal', identifier=i, backoff_time=backoff_times[1])\n",
    "        else:\n",
    "            worker = Worker(server, priority='low', identifier=i, backoff_time=backoff_times[2])\n",
    "        workers.append(worker)\n",
    "        worker.start()\n",
    "\n",
    "    for i in range(len(workers)):\n",
    "        server.state[f\"w{i}_backoff\"] = 0\n",
    "\n",
    "        prio = 0\n",
    "        if workers[i].priority == 'normal':\n",
    "            prio = -1\n",
    "        if workers[i].priority == 'high':\n",
    "            prio = -2\n",
    "        server.state[f\"w{i}_priority\"] = prio\n",
    "\n",
    "    # Start server threading, send workers to server\n",
    "    lifecycle_thread = threading.Thread(target=server_lifecycle, args=(server, workers, server_life))\n",
    "    lifecycle_thread.start()\n",
    "    lifecycle_thread.join()\n",
    "    return workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(workers, subdir=None, confirm_filename=False):\n",
    "    # Write worker information to CSV\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    csv_file = f\"workers_{timestamp}.csv\"\n",
    "    if subdir:\n",
    "        csv_file = os.path.join(subdir, f\"workers_{timestamp}.csv\")\n",
    "\n",
    "    header = [\"id\", \"priority\", \"total_work_time\", \"backoff_count\", \"connection_attempts\", \"connection_probability\", \"backoff_time\"]\n",
    "    # Open CSV file\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        # Loop through workers and write their information to the CSV\n",
    "        for i in range(len(workers)):\n",
    "            current_write = workers[i]\n",
    "            current_write.connection_probability = 0\n",
    "            if current_write.connection_attempts != 0:\n",
    "                current_write.connection_probability = (1-(current_write.backoff_count/current_write.connection_attempts))\n",
    "\n",
    "            worker_info = {\"id\": i, \"priority\": current_write.priority, \"total_work_time\": current_write.total_work_time, \"backoff_count\": current_write.backoff_count, \"connection_attempts\": current_write.connection_attempts, \"connection_probability\": current_write.connection_probability, \"backoff_time\": current_write.init_backoff_time}\n",
    "            writer.writerow(worker_info)\n",
    "    \n",
    "    if confirm_filename:\n",
    "        print(f\"Worker information written to {csv_file}\")\n",
    "    return csv_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "CONN_LIMIT = 5\n",
    "SIM_RUNS = 0 #1500\n",
    "worker_count = 20\n",
    "server_life = 30\n",
    "for i in range(SIM_RUNS):\n",
    "    HIGH_BACKOFF = random.uniform(0.1, 2)\n",
    "    MED_BACKOFF = random.uniform(.8, 4)\n",
    "    LOW_BACKOFF = random.uniform(3, 7)\n",
    "    print(f\"Simulation {i}: Worker count: {worker_count}, High: {HIGH_BACKOFF}, Med: {MED_BACKOFF}, Low: {LOW_BACKOFF}\")\n",
    "\n",
    "    # Instantiate server\n",
    "    server = Server(limit=CONN_LIMIT)\n",
    "    workers = simulate(server, worker_count=worker_count, server_life=server_life, backoff_times=[HIGH_BACKOFF, MED_BACKOFF, LOW_BACKOFF])\n",
    "    write_to_csv(workers)\n",
    "\n",
    "print(\"Simulation complete. Worker information written to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for neural network. \n",
    "# input features: high_target_connection_prob, normal_target_connection_prob, low_target_connection_prob, limit, high_num_workers, normal_num_workers, low_num_workers\n",
    "# output: high_backoff, normal_backoff, low_backoff\n",
    "def create_data(df):\n",
    "    # input features\n",
    "    high_target_connection_prob = df[df['priority'] == 'high']['connection_probability'].mean()\n",
    "    normal_target_connection_prob = df[df['priority'] == 'normal']['connection_probability'].mean()\n",
    "    low_target_connection_prob = df[df['priority'] == 'low']['connection_probability'].mean()\n",
    "    limit = 5\n",
    "    high_num_workers = len(df[df['priority'] == 'high'])\n",
    "    normal_num_workers = len(df[df['priority'] == 'normal'])\n",
    "    low_num_workers = len(df[df['priority'] == 'low'])\n",
    "\n",
    "    # output features (backoff times)\n",
    "    high_backoff = df[df['priority'] == 'high']['backoff_time'].mean()\n",
    "    normal_backoff = df[df['priority'] == 'normal']['backoff_time'].mean()\n",
    "    low_backoff = df[df['priority'] == 'low']['backoff_time'].mean()\n",
    "\n",
    "    #X = [high_target_connection_prob, normal_target_connection_prob, low_target_connection_prob, limit, high_num_workers, normal_num_workers, low_num_workers]\n",
    "    X = [high_target_connection_prob, normal_target_connection_prob, low_target_connection_prob] # let's try with fewer dimensions\n",
    "    Y = [high_backoff, normal_backoff, low_backoff]\n",
    "    \n",
    "    # scale the input features to be between 0 and 10\n",
    "    X = [x * 1 for x in X]\n",
    "    # scale the output features to be between 0 and 10\n",
    "    Y = [y * 1 for y in Y]\n",
    "    return X, Y\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.path.join(os.getcwd(), 'csmaca_traditional_data_out')\n",
    "\n",
    "# List all CSV files in the current directory\n",
    "csv_files = [file for file in os.listdir(current_dir) if file.endswith('.csv')]\n",
    "\n",
    "# Create an empty dataframe to store the compiled outputs\n",
    "compiled_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Read the CSV file into a dataframe\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # skip files that don't conform to the data format\n",
    "    if 'priority' not in df.columns or 'total_work_time' not in df.columns or 'backoff_count' not in df.columns or 'connection_attempts' not in df.columns or 'connection_probability' not in df.columns:\n",
    "        print(f\"Skipping {csv_file} as it does not conform to the data format.\")\n",
    "        continue\n",
    "    \n",
    "    # Perform the desired operations on the dataframe\n",
    "    X, Y = create_data(df)\n",
    "    \n",
    "    # Create a new row for the compiled dataframe\n",
    "    new_row = {'CSV File': csv_file, 'X': X, 'Y': Y}\n",
    "    \n",
    "    # Append the new row to the compiled dataframe\n",
    "    compiled_df = compiled_df.append(new_row, ignore_index=True)\n",
    "\n",
    "# Print the compiled dataframe\n",
    "print(compiled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train neural network with the compiled data\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "%pip install scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X = [high_target_connection_prob, normal_target_connection_prob, low_target_connection_prob, limit, high_num_workers, normal_num_workers, low_num_workers]\n",
    "#Y = [high_backoff, normal_backoff, low_backoff]\n",
    "X = compiled_df['X'].tolist()\n",
    "Y = compiled_df['Y'].tolist()\n",
    "\n",
    "# Scale features and targets\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "print(\"////////////////\")\n",
    "print(X_scaled)\n",
    "print(\"////////////////\")\n",
    "print(Y)\n",
    "y_scaled = scaler_y.fit_transform(Y)\n",
    "print(y_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # 3 input features\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc_out = nn.Linear(64, 3)  # 3 output features\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc_out(x)  # Linear output for regression\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "net = Net()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# save loss_data for plotting\n",
    "loss_data = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):  # Number of epochs\n",
    "    optimizer.zero_grad()   # Zero the gradient buffers\n",
    "    output = net(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    loss_data.append(loss.item())\n",
    "\n",
    "# plot loss_data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_data)\n",
    "plt.title('Loss Data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    predictions = net(X_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "# To use the model for prediction, remember to scale the input and inverse scale the output\n",
    "# generate predictions\n",
    "predictions_unscaled = scaler_y.inverse_transform(predictions.detach().numpy())\n",
    "\n",
    "# save model to disk timestamped\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_file = f\"model_{timestamp}.pth\"\n",
    "torch.save(net.state_dict(), model_file)\n",
    "print(f\"Model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from disk\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()\n",
    "\n",
    "new_probs = [0.3, 0.2, 0.05]\n",
    "print(f\"Querying model with probabilities: {new_probs}\")\n",
    "# scale new data to be between 0 and 10\n",
    "\n",
    "new_probs_scaled = scaler_X.transform([new_probs])\n",
    "new_probs_tensor = torch.tensor(new_probs_scaled, dtype=torch.float32)\n",
    "new_prediction = net(new_probs_tensor)\n",
    "new_prediction_unscaled = scaler_y.inverse_transform(new_prediction.detach().numpy())\n",
    "print(\"Answer (unscaled):\")\n",
    "print(new_prediction_unscaled)\n",
    "\n",
    "# scale prediction back to original scale\n",
    "new_prediction_scaled = scaler_y.transform(new_prediction_unscaled)\n",
    "print(\"Answer (scaled):\")\n",
    "print(new_prediction_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate with the above expectations\n",
    "\n",
    "# handy for testing\n",
    "#hardwired_backoffs = [2.3136058, 5.0742, 7.02123]\n",
    "#print(f\"To achieve the following connection probabilities: {[0.3, 0.2, 0.05]}, the backoff times should be: {hardwired_backoffs}\")\n",
    "\n",
    "print(f\"To achieve the following connection probabilities: {new_probs}, the backoff times should be: {new_prediction_unscaled[0].tolist()}\")\n",
    "# make directory for csv files\n",
    "dir_path = 'best_quantum_values'\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "NUM_SIMULATIONS = 1000\n",
    "for i in range(NUM_SIMULATIONS):\n",
    "    print(f\"TEST SIMULATION {i}\")\n",
    "    server = Server(limit=5)\n",
    "    #workers = simulate(server, worker_count=20, server_life=30, backoff_times=[1.3937324, 3.0184186, 3.6057389])\n",
    "    workers = simulate(server, worker_count=20, server_life=30, backoff_times=new_prediction_unscaled[0].tolist())\n",
    "    csv_file = write_to_csv(workers, subdir=dir_path)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Delete the CSV file if we don't want to pollute the repo\n",
    "    #os.remove(csv_file)\n",
    "    print(f\"Average connection probability for high priority workers: {df[df['priority'] == 'high']['connection_probability'].mean()}\")\n",
    "    print(f\"Average connection probability for normal priority workers: {df[df['priority'] == 'normal']['connection_probability'].mean()}\")\n",
    "    print(f\"Average connection probability for low priority workers: {df[df['priority'] == 'low']['connection_probability'].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
